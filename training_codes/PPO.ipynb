{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgMVh5uhMT7v+MYDqTlm/T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install gym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPFBq_Wm8ySr","executionInfo":{"status":"ok","timestamp":1676527046903,"user_tz":-540,"elapsed":5901,"user":{"displayName":"김호진","userId":"05277196388601250512"}},"outputId":"cf1b580c-d81a-4069-a660-bd938d844609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (6.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.12.1)\n"]}]},{"cell_type":"code","source":["import gym\n","import collections\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import tqdm\n","import matplotlib.pyplot as plt\n","from torch.distributions.categorical import Categorical"],"metadata":{"id":"c4a5WXgl8yc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.005\n","gamma = 0.98\n","lmbda = 0.95\n","eps_clip  = 0.1\n","K_epochs = 5\n","\n","policy_losses, value_losses = [], []"],"metadata":{"id":"mzP9fFT8p8XQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PPO(nn.Module):\n","  def __init__(self):\n","    super(PPO, self).__init__()\n","    self.data = []\n","    self.lmbda = lmbda # GAE(Generalized Advantage Estimation)에 쓰이는 계수\n","    self.gamma = gamma\n","    self.eps = eps_clip\n","\n","    self.fc1 = nn.Linear(4, 256)\n","    self.fc_pi = nn.Linear(256, 2) # action\n","    self.fc_v = nn.Linear(256, 1) # value\n","    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  # policy\n","  def pi(self, x, softmax_dim=0): # softmax_dim은 배치처리를 위한 \n","    x = F.relu(self.fc1(x))\n","    x = self.fc_pi(x)\n","    prob = F.softmax(x, dim=softmax_dim) # 어떤 디맨션 끼리 softmax해야하는지.\n","    return prob # action probability distribution\n","\n","  # value\n","  def v(self, x):\n","    x = F.relu(self.fc1(x))\n","    v = self.fc_v(x)\n","    return v\n","\n","  # put data : 데이터(trajectory) 넣는 부분\n","  def put_data(self, transition):\n","    self.data.append(transition)\n","\n","  # make batch\n","  def make_batch(self):\n","    s_list, a_list, r_list, s_prime_list, prob_a_list, done_mask_list = [], [], [], [], [], []\n","    \n","    for transition in self.data:\n","      s, a, r, s_prime, prob_a, done = transition\n","\n","      s_list.append(s)\n","      a_list.append([a]) # [a]는 파이토치 unsqueeze와 같다.\n","      r_list.append([r])\n","      s_prime_list.append(s_prime)\n","      prob_a_list.append([prob_a])\n","      done_mask = 0 if done else 1\n","      done_mask_list.append([done_mask])\n","\n","    s, a, r, s_prime, prob_a, done_mask = torch.tensor(s_list, dtype=torch.float), torch.tensor(a_list), torch.tensor(r_list),\\\n","                                  torch.tensor(s_prime_list, dtype=torch.float), torch.tensor(prob_a_list), \\\n","                                  torch.tensor(done_mask_list, dtype=torch.float)\n","    \n","    self.data = []\n","    return s, a, r, s_prime, prob_a, done_mask\n","\n","  def train(self):\n","    s, a, r, s_prime, prob_a, done_mask = self.make_batch()\n","\n","    for i in range(K_epochs):\n","      # 한번 갔으니까 가서 얻은 리워드 + 간 곳에서 기대하는 가치( v(s) ) 인데 한번 갔으니 감마 곱하기\n","      td_target = r + gamma * self.v(s_prime) * done_mask\n","      delta = td_target - self.v(s) # 차이가 어드밴티지의 정의\n","      delta = delta.detach().numpy() # 이해한 원래의 Advantage 값. GAE를 위해서 delta로 적음\n","\n","      # 델타를 뒤에서부터 보면서 gamma랑 lamda를 곱하고\n","      # 계속해서 더해나가면서 저장\n","      advantage_list = []\n","      advantage = 0.0\n","      for delta_t in delta[::-1]:\n","        advantage = gamma * lmbda * advantage + delta_t[0] # 그럼 먼저들어간것들은 람다랑 감마가 계속 곱해짐\n","        advantage_list.append([advantage]) \n","      advantage_list.reverse() # 위에서 [::-1]로 거꾸로 쌓았기 때문에\n","      advantage = torch.tensor(advantage_list, dtype=torch.float)\n","      \n","      # 확률 뽑고\n","      pi = self.pi(s, softmax_dim=1)\n","      # 그 확률 중에서 실제 했던 액션의 최신 policy에서의 확률 구하고.\n","      pi_a = pi.gather(1, a)\n","      # ratio 계산. -> clipped loss function에서 (파이 세타 / 파이 세타 올드) 한 그 ratio. 근데 a/b == exp(log(a) - log(b))이기도 해서 이렇게 적는다. 더 좋다고함\n","      ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a)) # prob_a는 경험 쌓을 때 했던 액션의 확률. pi_a는 그 액션이 현재 policy에서의 확률\n","\n","      # ratio * advantage = min의 좌측\n","      surr1 = ratio * advantage\n","      # min의 우측 : clip 한거 clip(ratio, 1-e, 1+e,) * A\n","      surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps)*advantage # clip은 ratio가 1-e 보다 작으면 1-e로, 1+e 보다 크면 1+e로 만드는 것.\n","      # 위 두개 써서 계산 + td target과 v의 smooth error loss => 작아져야함\n","      # 왼쪽은 policy loss, 오른쪽은 value loss. \n","      loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s), td_target.detach())\n","      value_losses.append(F.smooth_l1_loss(self.v(s), td_target.detach()))\n","      policy_losses.append(-torch.min(surr1, surr2))\n","\n","      self.optimizer.zero_grad()\n","      loss.mean().backward()\n","      self.optimizer.step()\n"],"metadata":{"id":"b9UxuNGlX-5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrwBGkHm8NLQ"},"outputs":[],"source":["env = gym.make('CartPole-v1')\n","T = 20 # 몇 타임스텝 동안 trajectories를 모을지.\n","model = PPO()\n","score = 0.0\n","print_interval = 20\n","num_episodes = 10000"]},{"cell_type":"code","source":["for episode in range(num_episodes):\n","  # Reset environment and get first new onservation\n","  state = env.reset()\n","  done = False\n","\n","  # The Q-Table learning algorithm\n","  while not done:\n","    # collect during T timesteps\n","    for t in range(T):\n","      prob = model.pi(torch.from_numpy(state).float()) # 모델한테 확률 뱉으라고 하고\n","      m = Categorical(prob) # 확률을 categorical 변수로 만들어서\n","      a = m.sample().item() # action sampling.\n","      s_prime, r, done, _ = env.step(a) # action을 던진다. s_prime은 바뀐 state\n","      model.put_data((state, a, r/100.0, s_prime, prob[a].item(), done)) # r은 값이 커서 그냥 100으로 나눔. 학습이 잘되더라\n","      # prob[a].item()은 왜 갑자기 저장하지? -> 실제 내가 했던 action의 확률값. PPO에 ratio 계산을 하는데 그 때 필요하다.\n","      state = s_prime\n","\n","      score += r\n","      if done:\n","        break\n","    \n","    model.train()\n","  if episode%print_interval==0 and episode!=0:\n","      print(\"# of episode :{}, avg score : {:.1f}\".format(episode, score/print_interval))\n","      score = 0.0"],"metadata":{"id":"xSZEDllk8zoE","colab":{"base_uri":"https://localhost:8080/","height":860},"executionInfo":{"status":"error","timestamp":1676527131134,"user_tz":-540,"elapsed":84234,"user":{"displayName":"김호진","userId":"05277196388601250512"}},"outputId":"5b162bda-16ea-4cf7-d0e1-b373f2339250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# of episode :20, avg score : 11.8\n","# of episode :40, avg score : 9.4\n","# of episode :60, avg score : 10.6\n","# of episode :80, avg score : 10.3\n","# of episode :100, avg score : 9.8\n","# of episode :120, avg score : 10.1\n","# of episode :140, avg score : 10.2\n","# of episode :160, avg score : 12.8\n","# of episode :180, avg score : 29.6\n","# of episode :200, avg score : 44.9\n","# of episode :220, avg score : 82.4\n","# of episode :240, avg score : 84.7\n","# of episode :260, avg score : 81.0\n","# of episode :280, avg score : 110.4\n","# of episode :300, avg score : 138.2\n","# of episode :320, avg score : 108.8\n","# of episode :340, avg score : 156.3\n","# of episode :360, avg score : 204.8\n","# of episode :380, avg score : 133.9\n","# of episode :400, avg score : 128.8\n","# of episode :420, avg score : 196.4\n","# of episode :440, avg score : 248.0\n","# of episode :460, avg score : 179.5\n","# of episode :480, avg score : 464.7\n","# of episode :500, avg score : 234.8\n","# of episode :520, avg score : 120.3\n","# of episode :540, avg score : 118.0\n","# of episode :560, avg score : 118.8\n","# of episode :580, avg score : 131.3\n","# of episode :600, avg score : 164.2\n","# of episode :620, avg score : 129.9\n","# of episode :640, avg score : 132.9\n","# of episode :660, avg score : 206.9\n","# of episode :680, avg score : 154.8\n","# of episode :700, avg score : 196.9\n","# of episode :720, avg score : 417.8\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ac32fa09b244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 모델한테 확률 뱉으라고 하고\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 확률을 categorical 변수로 만들어서\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# action sampling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# action을 던진다. s_prime은 바뀐 state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# r은 값이 커서 그냥 100으로 나눔. 학습이 잘되더라\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["m = nn.Softmax(dim=0)\n","input = torch.randn(2, 3)\n","output = m(input)"],"metadata":{"id":"8I33dmKI98eC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output"],"metadata":{"id":"-JI8FO0yX3XA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676527153214,"user_tz":-540,"elapsed":3,"user":{"displayName":"김호진","userId":"05277196388601250512"}},"outputId":"6ae2ed02-b4ce-4770-9ec2-3c9ae01982c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0957, 0.1954, 0.8602],\n","        [0.9043, 0.8046, 0.1398]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# import gym\n","# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import torch.optim as optim\n","# from torch.distributions import Categorical\n","\n","# #Hyperparameters\n","# learning_rate = 0.0005\n","# gamma         = 0.98\n","# lmbda         = 0.95\n","# eps_clip      = 0.1\n","# K_epoch       = 3\n","# T_horizon     = 20\n","\n","# class PPO(nn.Module):\n","#     def __init__(self):\n","#         super(PPO, self).__init__()\n","#         self.data = []\n","        \n","#         self.fc1   = nn.Linear(4,256)\n","#         self.fc_pi = nn.Linear(256,2)\n","#         self.fc_v  = nn.Linear(256,1)\n","#         self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","\n","#     def pi(self, x, softmax_dim = 0):\n","#         x = F.relu(self.fc1(x))\n","#         x = self.fc_pi(x)\n","#         prob = F.softmax(x, dim=softmax_dim)\n","#         return prob\n","    \n","#     def v(self, x):\n","#         x = F.relu(self.fc1(x))\n","#         v = self.fc_v(x)\n","#         return v\n","      \n","#     def put_data(self, transition):\n","#         self.data.append(transition)\n","        \n","#     def make_batch(self):\n","#         s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n","#         for transition in self.data:\n","#             s, a, r, s_prime, prob_a, done = transition\n","            \n","#             s_lst.append(s)\n","#             a_lst.append([a])\n","#             r_lst.append([r])\n","#             s_prime_lst.append(s_prime)\n","#             prob_a_lst.append([prob_a])\n","#             done_mask = 0 if done else 1\n","#             done_lst.append([done_mask])\n","            \n","#         s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","#                                           torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","#                                           torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n","#         self.data = []\n","#         return s, a, r, s_prime, done_mask, prob_a\n","        \n","#     def train_net(self):\n","#         s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n","\n","#         for i in range(K_epoch):\n","#             td_target = r + gamma * self.v(s_prime) * done_mask\n","#             delta = td_target - self.v(s)\n","#             delta = delta.detach().numpy()\n","\n","#             advantage_lst = []\n","#             advantage = 0.0\n","#             for delta_t in delta[::-1]:\n","#                 advantage = gamma * lmbda * advantage + delta_t[0]\n","#                 advantage_lst.append([advantage])\n","#             advantage_lst.reverse()\n","#             advantage = torch.tensor(advantage_lst, dtype=torch.float)\n","\n","#             pi = self.pi(s, softmax_dim=1)\n","#             pi_a = pi.gather(1,a)\n","#             ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n","\n","#             surr1 = ratio * advantage\n","#             surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n","#             loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n","\n","#             self.optimizer.zero_grad()\n","#             loss.mean().backward()\n","#             self.optimizer.step()\n","        \n","# env = gym.make('CartPole-v1')\n","# model = PPO()\n","# score = 0.0\n","# print_interval = 20\n","\n","# for n_epi in range(10000):\n","#     s = env.reset()\n","#     done = False\n","#     while not done:\n","#         for t in range(T_horizon):\n","#             prob = model.pi(torch.from_numpy(s).float())\n","#             m = Categorical(prob)\n","#             a = m.sample().item()\n","#             s_prime, r, done, info = env.step(a)\n","\n","#             model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n","#             s = s_prime\n","\n","#             score += r\n","#             if done:\n","#                 break\n","\n","#         model.train_net()\n","\n","#     if n_epi%print_interval==0 and n_epi!=0:\n","#         print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n","#         score = 0.0\n","\n","# env.close()"],"metadata":{"id":"ouaMh0lWpn0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w0CDZcB7rvht"},"execution_count":null,"outputs":[]}]}